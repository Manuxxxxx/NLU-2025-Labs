{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cbca6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emanuele/.conda/envs/nlu25/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1 - Training: 100%|███████████████████████████████████████| 2240/2240 [08:18<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Loss: 3045.8029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Training: 100%|███████████████████████████████████████| 2240/2240 [06:22<00:00,  5.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Loss: 1289.5969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Training: 100%|███████████████████████████████████████| 2240/2240 [05:54<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Loss: 908.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Evaluating dev: 100%|███████████████████████████████████| 249/249 [00:10<00:00, 23.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Accuracy: 0.9639 | Slot F1: 0.9172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Training: 100%|███████████████████████████████████████| 2240/2240 [05:54<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Loss: 707.9372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Training: 100%|███████████████████████████████████████| 2240/2240 [05:54<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Loss: 621.4571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Training: 100%|███████████████████████████████████████| 2240/2240 [08:04<00:00,  4.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Loss: 536.1164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Evaluating dev: 100%|███████████████████████████████████| 249/249 [00:14<00:00, 16.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Accuracy: 0.9759 | Slot F1: 0.9461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Training: 100%|███████████████████████████████████████| 2240/2240 [06:37<00:00,  5.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Loss: 514.2330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Training: 100%|███████████████████████████████████████| 2240/2240 [05:54<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Loss: 502.8183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Training: 100%|███████████████████████████████████████| 2240/2240 [05:54<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Loss: 344.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Evaluating dev: 100%|███████████████████████████████████| 249/249 [00:10<00:00, 23.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intent Accuracy: 0.9900 | Slot F1: 0.9548\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "eval_loop() missing 1 required positional argument: 'epoch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 209\u001b[0m\n\u001b[1;32m    206\u001b[0m         eval_loop(model, dev_loader, epoch, tag\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Final test evaluation\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;66;03m# Save model\u001b[39;00m\n\u001b[1;32m    212\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_bin\u001b[39m\u001b[38;5;124m\"\u001b[39m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: eval_loop() missing 1 required positional argument: 'epoch'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertModel, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import os\n",
    "\n",
    "# CONFIG\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 9\n",
    "PAD_TOKEN = -100\n",
    "\n",
    "id_run = time.strftime(\"%d%m%y_%H%M%S\")\n",
    "writer = SummaryWriter(log_dir='runs/CustomBert_' + id_run)\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "def load_data(path):\n",
    "    with open(path) as f:\n",
    "        return json.load(f)\n",
    "\n",
    "train_dev_data = load_data(os.path.join('dataset', 'ATIS', 'train.json'))\n",
    "test_data = load_data(os.path.join('dataset', 'ATIS', 'test.json'))\n",
    "data_raw = test_data + train_dev_data\n",
    "\n",
    "intents = sorted({item['intent'] for item in data_raw})\n",
    "intent2id = {label: i for i, label in enumerate(intents)}\n",
    "id2intent = {i: label for label, i in intent2id.items()}\n",
    "\n",
    "slots = sorted({slot for item in data_raw for slot in item['slots']})\n",
    "slot2id = {label: i for i, label in enumerate(slots)}\n",
    "id2slot = {i: label for label, i in slot2id.items()}\n",
    "\n",
    "class ATISDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, intent2id, slot2id, max_len=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "        self.intent2id = intent2id\n",
    "        self.slot2id = slot2id\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        \n",
    "        # Ensure tokens are split correctly\n",
    "        tokens = item['utterance']\n",
    "        if isinstance(tokens, str):\n",
    "            tokens = tokens.split()  # Split string into list of tokens\n",
    "\n",
    "        intent = item['intent']\n",
    "        slots = item['slots']\n",
    "\n",
    "        encoding = self.tokenizer(tokens,\n",
    "                                is_split_into_words=True,\n",
    "                                return_offsets_mapping=True,\n",
    "                                padding='max_length',\n",
    "                                truncation=True,\n",
    "                                max_length=self.max_len)\n",
    "        \n",
    "        word_ids = encoding.word_ids()\n",
    "        slot_labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                slot_labels.append(PAD_TOKEN)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                slot_labels.append(self.slot2id[slots[word_idx]])\n",
    "            else:\n",
    "                slot_labels.append(PAD_TOKEN)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoding['input_ids']),\n",
    "            'attention_mask': torch.tensor(encoding['attention_mask']),\n",
    "            'token_type_ids': torch.tensor(encoding['token_type_ids']),\n",
    "            'intent_label': torch.tensor(self.intent2id[intent]),\n",
    "            'slot_labels': torch.tensor(slot_labels)\n",
    "        }\n",
    "\n",
    "\n",
    "class JointBERT(nn.Module):\n",
    "    def __init__(self, model_name, num_intents, num_slots):\n",
    "        super(JointBERT, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(model_name)\n",
    "        self.intent_classifier = nn.Linear(self.bert.config.hidden_size, num_intents)\n",
    "        self.slot_classifier = nn.Linear(self.bert.config.hidden_size, num_slots)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask=attention_mask,\n",
    "                            token_type_ids=token_type_ids)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        sequence_output = outputs.last_hidden_state\n",
    "        return self.intent_classifier(pooled_output), self.slot_classifier(sequence_output)\n",
    "\n",
    "def compute_slot_f1(preds, trues):\n",
    "    correct, total_pred, total_true = 0, 0, 0\n",
    "    for p_seq, t_seq in zip(preds, trues):\n",
    "        for p, t in zip(p_seq, t_seq):\n",
    "            total_pred += 1\n",
    "            total_true += 1\n",
    "            if p == t:\n",
    "                correct += 1\n",
    "    precision = correct / total_pred if total_pred > 0 else 0\n",
    "    recall = correct / total_true if total_true > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0\n",
    "    return f1\n",
    "\n",
    "# Prepare data\n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "train_data, dev_data = train_test_split(train_dev_data, test_size=0.1, random_state=42)\n",
    "train_dataset = ATISDataset(train_data, tokenizer, intent2id, slot2id, MAX_LEN)\n",
    "dev_dataset = ATISDataset(dev_data, tokenizer, intent2id, slot2id, MAX_LEN)\n",
    "test_dataset = ATISDataset(test_data, tokenizer, intent2id, slot2id, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Initialize model and loss\n",
    "model = JointBERT(MODEL_NAME, len(intent2id), len(slot2id)).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "intent_loss_fn = nn.CrossEntropyLoss()\n",
    "slot_loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
    "\n",
    "def train_loop(model, train_loader, optimizer, intent_loss_fn, slot_loss_fn, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1} - Training\", ncols=100):  # Add tqdm here\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        intent_labels = batch['intent_label'].to(device)\n",
    "        slot_labels = batch['slot_labels'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        intent_logits, slot_logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "        intent_loss = intent_loss_fn(intent_logits, intent_labels)\n",
    "        slot_loss = slot_loss_fn(slot_logits.view(-1, len(slot2id)), slot_labels.view(-1))\n",
    "        loss = intent_loss + slot_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} - Loss: {total_loss:.4f}\")\n",
    "    writer.add_scalar('Loss/train', total_loss, epoch)\n",
    "\n",
    "\n",
    "# Evaluation loop function\n",
    "def eval_loop(model, dev_loader, epoch, tag=\"dev\"):\n",
    "    model.eval()\n",
    "    intent_preds, intent_true = [], []\n",
    "    slot_preds, slot_true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dev_loader, desc=f\"Epoch {epoch+1} - Evaluating {tag}\", ncols=100):  # Add tqdm here\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            intent_labels = batch['intent_label'].to(device)\n",
    "            slot_labels = batch['slot_labels'].to(device)\n",
    "\n",
    "            intent_logits, slot_logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            # Intent predictions\n",
    "            preds = torch.argmax(intent_logits, dim=1)\n",
    "            intent_preds.extend(preds.cpu().numpy())\n",
    "            intent_true.extend(intent_labels.cpu().numpy())\n",
    "\n",
    "            # Slot predictions\n",
    "            slot_pred_ids = torch.argmax(slot_logits, dim=2)\n",
    "            for i in range(slot_labels.size(0)):\n",
    "                true_labels = []\n",
    "                pred_labels = []\n",
    "                for j in range(slot_labels.size(1)):\n",
    "                    if slot_labels[i][j] != PAD_TOKEN:\n",
    "                        true_labels.append(id2slot[slot_labels[i][j].item()])\n",
    "                        pred_labels.append(id2slot[slot_pred_ids[i][j].item()])\n",
    "                slot_true.append(true_labels)\n",
    "                slot_preds.append(pred_labels)\n",
    "\n",
    "    # Calculate and print metrics\n",
    "    intent_acc = accuracy_score(intent_true, intent_preds)\n",
    "    slot_f1 = compute_slot_f1(slot_true, slot_preds)  # Use your own F1 calculation function\n",
    "    print(f\"Intent Accuracy: {intent_acc:.4f} | Slot F1: {slot_f1:.4f}\")\n",
    "\n",
    "    # Log metrics to TensorBoard\n",
    "    writer.add_scalar(f\"{tag}/Intent Accuracy\", intent_acc, epoch)\n",
    "    writer.add_scalar(f\"{tag}/Slot F1\", slot_f1, epoch)\n",
    "\n",
    "def test_loop(model, test_loader, tag=\"test\"):\n",
    "    model.eval()\n",
    "    intent_preds, intent_true = [], []\n",
    "    slot_preds, slot_true = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=f\"Evaluating {tag}\", ncols=100):  # Add tqdm here\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            intent_labels = batch['intent_label'].to(device)\n",
    "            slot_labels = batch['slot_labels'].to(device)\n",
    "\n",
    "            intent_logits, slot_logits = model(input_ids, attention_mask, token_type_ids)\n",
    "\n",
    "            # Intent predictions\n",
    "            preds = torch.argmax(intent_logits, dim=1)\n",
    "            intent_preds.extend(preds.cpu().numpy())\n",
    "            intent_true.extend(intent_labels.cpu().numpy())\n",
    "\n",
    "            # Slot predictions\n",
    "            slot_pred_ids = torch.argmax(slot_logits, dim=2)\n",
    "            for i in range(slot_labels.size(0)):\n",
    "                true_labels = []\n",
    "                pred_labels = []\n",
    "                for j in range(slot_labels.size(1)):\n",
    "                    if slot_labels[i][j] != PAD_TOKEN:\n",
    "                        true_labels.append(id2slot[slot_labels[i][j].item()])\n",
    "                        pred_labels.append(id2slot[slot_pred_ids[i][j].item()])\n",
    "                slot_true.append(true_labels)\n",
    "                slot_preds.append(pred_labels)\n",
    "\n",
    "    # Calculate and print metrics\n",
    "    intent_acc = accuracy_score(intent_true, intent_preds)\n",
    "    slot_f1 = compute_slot_f1(slot_true, slot_preds)  # Use your own F1 calculation function\n",
    "    print(f\"Intent Accuracy: {intent_acc:.4f} | Slot F1: {slot_f1:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loop(model, train_loader, optimizer, intent_loss_fn, slot_loss_fn, epoch)\n",
    "    if (epoch + 1) % 3 == 0:\n",
    "        eval_loop(model, dev_loader, epoch, tag=\"dev\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loop(model, test_loader, tag=\"test\")\n",
    "\n",
    "# Save model\n",
    "os.makedirs(\"model_bin\", exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"model_bin/joint_bert_model_{id_run}.pt\")\n",
    "print(f\"Model saved: model_bin/joint_bert_model_{id_run}.pt\")\n",
    "writer.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
